{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k=1: 100.0\n",
      "Accuracy for k=total_instances: 50.0\n"
     ]
    }
   ],
   "source": [
    "cats_instances = 50\n",
    "dogs_instances = 30\n",
    "cows_instances = 20\n",
    "total_instances = cats_instances + dogs_instances + cows_instances\n",
    "\n",
    "# k=1\n",
    "acc1 = 100.0\n",
    "\n",
    "# k=total_instances\n",
    "acck = (cats_instances / total_instances) * 100\n",
    "\n",
    "print(\"Accuracy for k=1:\", acc1)\n",
    "print(\"Accuracy for k=total_instances:\", acck)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navie Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.priors = None\n",
    "        self.means = None\n",
    "        self.stds = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "\n",
    "        # Calculate priors\n",
    "        self.priors = np.zeros(n_classes)\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            self.priors[idx] = np.sum(y == c) / n_samples\n",
    "            print(f\"Class {c}'s Prior(P) is {self.priors[idx]:.3f}\")\n",
    "\n",
    "        # Calculate means and standard deviations for each feature in each class\n",
    "        self.means = np.zeros((n_classes, n_features))\n",
    "        self.stds = np.zeros((n_classes, n_features))\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.means[idx, :] = np.mean(X_c, axis=0)\n",
    "            self.stds[idx, :] = np.std(X_c, axis=0)\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            for idx_f in range(n_features):\n",
    "                print(f\"Class {c}'s Feature {idx_f} Mean is {self.means[idx,idx_f]:.3f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.array([self._predict(x) for x in X])\n",
    "        print(f\"{X}'s Predict Class is {y_pred}\")\n",
    "        return y_pred\n",
    "\n",
    "    def _predict(self, x):\n",
    "        posteriors = []\n",
    "\n",
    "        # Calculate posterior probabilities for each class\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            prior = np.log(self.priors[idx])\n",
    "            likelihood = np.sum(np.log(norm.pdf(x, self.means[idx], self.stds[idx])))\n",
    "            print(f\"For {x},class {c}'s Likelihood is {likelihood:.3f}\")\n",
    "            posterior = prior + likelihood\n",
    "            print(f\"For {x},class {c}'s Posterior(L) is {posterior:.3f}\")\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        # Select the class with the highest posterior probability\n",
    "        return self.classes[np.argmax(posteriors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"demo-bayes.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1].to_numpy()\n",
    "y = df.iloc[:,-1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1's Prior(P) is 0.400\n",
      "Class 2's Prior(P) is 0.333\n",
      "Class 3's Prior(P) is 0.267\n",
      "Class 1's Feature 0 Mean is 1.543\n",
      "Class 1's Feature 1 Mean is 1.968\n",
      "Class 2's Feature 0 Mean is 1.792\n",
      "Class 2's Feature 1 Mean is 2.584\n",
      "Class 3's Feature 0 Mean is 1.962\n",
      "Class 3's Feature 1 Mean is 0.932\n"
     ]
    }
   ],
   "source": [
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([[2.0,2.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For [2.  2.1],class 1's Likelihood is -15.672\n",
      "For [2.  2.1],class 1's Posterior(L) is -16.588\n",
      "For [2.  2.1],class 2's Likelihood is -1.760\n",
      "For [2.  2.1],class 2's Posterior(L) is -2.859\n",
      "For [2.  2.1],class 3's Likelihood is -85.589\n",
      "For [2.  2.1],class 3's Posterior(L) is -86.911\n",
      "[[2.  2.1]]'s Predict Class is [2]\n"
     ]
    }
   ],
   "source": [
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.read_csv(\"confusion_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = df_cm.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 3.000, TN: 90.000, FP: 5.000, FN: 2.000\n",
      "Precision: 0.375, Accuracy: 0.930, Recall: 0.600, F1: 0.462\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 提取混淆矩阵中的计数\n",
    "TP, FN, FP, TN = cm.ravel()\n",
    "\n",
    "# 计算四个度量\n",
    "precision = TP / (TP + FP)\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "recall = TP / (TP + FN)\n",
    "f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "# 输出结果\n",
    "print(\"TP: {:.3f}, TN: {:.3f}, FP: {:.3f}, FN: {:.3f}\".format(TP, TN, FP, FN))\n",
    "print(\"Precision: {:.3f}, Accuracy: {:.3f}, Recall: {:.3f}, F1: {:.3f}\".format(precision, accuracy, recall, f1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手动计算 Precision: 0.80, Accuracy: 0.75, Recall: 0.80, F1: 0.80\n",
      "sklearn计算 Precision: 0.80, Accuracy: 0.75, Recall: 0.80, F1: 0.80\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 模型预测结果\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 1]\n",
    "y_pred = [1, 1, 1, 0, 0, 1, 0, 1]\n",
    "\n",
    "# 计算混淆矩阵\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# 提取混淆矩阵中的计数\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "# 计算四个度量\n",
    "precision = TP / (TP + FP)\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "recall = TP / (TP + FN)\n",
    "f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "# 使用 sklearn 计算四个度量进行验证\n",
    "precision_skl = precision_score(y_true, y_pred)\n",
    "accuracy_skl = accuracy_score(y_true, y_pred)\n",
    "recall_skl = recall_score(y_true, y_pred)\n",
    "f1_skl = f1_score(y_true, y_pred)\n",
    "\n",
    "# 输出结果\n",
    "print(\"手动计算 Precision: {:.2f}, Accuracy: {:.2f}, Recall: {:.2f}, F1: {:.2f}\".format(precision, accuracy, recall, f1))\n",
    "print(\"sklearn计算 Precision: {:.2f}, Accuracy: {:.2f}, Recall: {:.2f}, F1: {:.2f}\".format(precision_skl, accuracy_skl, recall_skl, f1_skl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training a k-nn classifier just involves storing the data, whereas making a prediction involves calculating the distance from one point to all other points.\n",
    "\n",
    "training a k-nn clasifier just involves storing the data, which means it takes the same time for any value of k. The value of k makes a difference on prediction time, however.\n",
    "\n",
    "we should choose the hyperparameters based on perfomance on a validation set, not the held-out test set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"line_regrassion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1].to_numpy().reshape(-1)\n",
    "y = df.iloc[:,-1].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grad is -0.157 and the Inter is 63.072\n"
     ]
    }
   ],
   "source": [
    "gradient, intercept = np.polyfit(X, y, 1)\n",
    "print(f\"The grad is {gradient:.3f} and the Inter is {intercept:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, n_clusters, max_iter=300, random_state=None,init_centroids=None):\n",
    "        self._init_centroids = np.array(init_centroids)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        if self._init_centroids is not None:\n",
    "            if len(self._init_centroids) != n_clusters:\n",
    "                raise RuntimeError(\"Please check your input\")\n",
    "\n",
    "    def get_init_centroids(self, X):\n",
    "        np.random.seed(self.random_state)\n",
    "        random_idx = np.random.permutation(X.shape[0])\n",
    "        centroids = X[random_idx[:self.n_clusters]]\n",
    "        return centroids\n",
    "\n",
    "    def _update_centroids(self, X, labels):\n",
    "        centroids = np.zeros((self.n_clusters, X.shape[1]))\n",
    "        for k in range(self.n_clusters):\n",
    "            centroids[k, :] = X[labels == k].mean(axis=0)\n",
    "        return centroids\n",
    "\n",
    "    def _labels_closest_centroids(self, X, centroids):\n",
    "        distances = np.zeros((X.shape[0], self.n_clusters))\n",
    "        for k in range(self.n_clusters):\n",
    "            distances[:, k] = np.linalg.norm(X - centroids[k], axis=1)\n",
    "        labels = distances.argmin(axis=1)\n",
    "        return labels\n",
    "\n",
    "    def fit(self, X):\n",
    "        if self._init_centroids is None:\n",
    "            self.centroids_ = self.get_init_centroids(X)\n",
    "        else:\n",
    "            self.centroids_ = self._init_centroids\n",
    "        for _ in range(self.max_iter):\n",
    "            print(f\"{_+1} iter:\")\n",
    "            prev_centroids = self.centroids_\n",
    "            print(f\"Before Centroids:{prev_centroids}\")\n",
    "            self.labels_ = self._labels_closest_centroids(X, prev_centroids)\n",
    "            for k in range(self.n_clusters):\n",
    "                print(f\"Indices in label {k}: {np.where(self.labels_ == k)[0]}\")\n",
    "            self.centroids_ = self._update_centroids(X, self.labels_)\n",
    "            print(f\"After Centroids:{self.centroids_},Labels: {Counter(self.labels_)}\")\n",
    "            if np.allclose(self.centroids_, prev_centroids):\n",
    "                break\n",
    "        print(f\"Finish trainning using {_} times iter\")\n",
    "        print(Counter(self.labels_))\n",
    "\n",
    "    def predict(self, X):\n",
    "        labels = self._labels_closest_centroids(X, self.centroids_)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"demo_cluster.csv\",header=None)\n",
    "X = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2,init_centroids=[[0,6],[4,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iter:\n",
      "Before Centroids:[[0 6]\n",
      " [4 2]]\n",
      "Indices in label 0: [1 2 4 5 7]\n",
      "Indices in label 1: [0 3 6]\n",
      "After Centroids:[[2.         4.4       ]\n",
      " [2.33333333 3.33333333]],Labels: Counter({0: 5, 1: 3})\n",
      "2 iter:\n",
      "Before Centroids:[[2.         4.4       ]\n",
      " [2.33333333 3.33333333]]\n",
      "Indices in label 0: [2 4 5 6 7]\n",
      "Indices in label 1: [0 1 3]\n",
      "After Centroids:[[2.6        4.8       ]\n",
      " [1.33333333 2.66666667]],Labels: Counter({0: 5, 1: 3})\n",
      "3 iter:\n",
      "Before Centroids:[[2.6        4.8       ]\n",
      " [1.33333333 2.66666667]]\n",
      "Indices in label 0: [4 5 6 7]\n",
      "Indices in label 1: [0 1 2 3]\n",
      "After Centroids:[[3.   5.  ]\n",
      " [1.25 3.  ]],Labels: Counter({1: 4, 0: 4})\n",
      "4 iter:\n",
      "Before Centroids:[[3.   5.  ]\n",
      " [1.25 3.  ]]\n",
      "Indices in label 0: [4 6 7]\n",
      "Indices in label 1: [0 1 2 3 5]\n",
      "After Centroids:[[3.33333333 5.33333333]\n",
      " [1.4        3.2       ]],Labels: Counter({1: 5, 0: 3})\n",
      "5 iter:\n",
      "Before Centroids:[[3.33333333 5.33333333]\n",
      " [1.4        3.2       ]]\n",
      "Indices in label 0: [4 6 7]\n",
      "Indices in label 1: [0 1 2 3 5]\n",
      "After Centroids:[[3.33333333 5.33333333]\n",
      " [1.4        3.2       ]],Labels: Counter({1: 5, 0: 3})\n",
      "Finish trainning using 4 times iter\n",
      "Counter({1: 5, 0: 3})\n"
     ]
    }
   ],
   "source": [
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(y):\n",
    "    \"\"\"\n",
    "    计算熵\n",
    "    \"\"\"\n",
    "    n_samples = len(y)\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / n_samples\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_feature_name(clf, feature_names):\n",
    "    \"\"\"\n",
    "    获取最佳分裂特征\n",
    "    \"\"\"\n",
    "    importances = clf.feature_importances_\n",
    "    # 返回最重要特征的名称\n",
    "    max_index = importances.argmax()\n",
    "    return feature_names[max_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_split_feature_name(clf, feature_names):\n",
    "    best_feature_index = clf.tree_.feature[0]\n",
    "    return feature_names[best_feature_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"demo-clf.csv\")\n",
    "df_new = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in df:\n",
    "    le = LabelEncoder()\n",
    "    df_new[d] = le.fit_transform(df[d])\n",
    "df = df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Style</th>\n",
       "      <th>Colour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subject  Style  Colour\n",
       "0        1      0       0\n",
       "1        0      0       2\n",
       "2        0      1       1\n",
       "3        1      1       1\n",
       "4        2      1       1\n",
       "5        2      0       2\n",
       "6        0      1       2\n",
       "7        2      1       2"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root entropy is 1.0\n",
      "Most Important feature is Subject, and  the best feature to split on first is Colour\n",
      "Final depth is 3\n"
     ]
    }
   ],
   "source": [
    "# Create a decision tree classifier and fit it to the dataset\n",
    "print(f\"root entropy is {get_entropy(y)}\")\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "clf.fit(X.to_numpy(), y.to_numpy())\n",
    "print(f\"Most Important feature is {get_best_feature_name(clf,list(X.columns))}, and  the best feature to split on first is {get_best_split_feature_name(clf,list(X.columns))}\")\n",
    "print(f\"Final depth is {clf.get_depth()-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Source.gv.pdf'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export the decision tree to a Graphviz format\n",
    "dot_data = export_graphviz(clf, out_file=None, feature_names=X.columns, filled=True, rounded=True)\n",
    "\n",
    "# Create a Graphviz object to visualize the decision tree\n",
    "graph = graphviz.Source(dot_data)\n",
    "\n",
    "# Display the decision tree\n",
    "graph.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        根据基尼系数计算最佳分割特征和阈值\n",
    "        \"\"\"\n",
    "        best_gini = np.inf\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                mask = X[:, feature] <= threshold\n",
    "                y_left, y_right = y[mask], y[~mask]\n",
    "                gini = self._gini(y_left, y_right)\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _gini(self, y_left, y_right):\n",
    "        \"\"\"\n",
    "        计算基尼系数\n",
    "        \"\"\"\n",
    "        n_left, n_right = len(y_left), len(y_right)\n",
    "        n_total = n_left + n_right\n",
    "        gini_left = 1 - np.sum((np.bincount(y_left) / n_left) ** 2)\n",
    "        gini_right = 1 - np.sum((np.bincount(y_right) / n_right) ** 2)\n",
    "        return (n_left * gini_left + n_right * gini_right) / n_total\n",
    "\n",
    "    def _grow_tree(self, X, y, depth):\n",
    "        \"\"\"\n",
    "        递归地生成决策树\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        if n_labels == 1 or n_samples == 0 or depth == self.max_depth:\n",
    "            return self._create_leaf(y)\n",
    "        else:\n",
    "            feature, threshold = self._best_split(X, y)\n",
    "            mask = X[:, feature] <= threshold\n",
    "            left = self._grow_tree(X[mask], y[mask], depth + 1)\n",
    "            right = self._grow_tree(X[~mask], y[~mask], depth + 1)\n",
    "            return self._create_node(feature, threshold, left, right)\n",
    "\n",
    "    def _create_leaf(self, y):\n",
    "        return {'label': np.argmax(np.bincount(y))}\n",
    "\n",
    "    def _create_node(self, feature, threshold, left, right):\n",
    "        return {'feature': feature, 'threshold': threshold, 'left': left, 'right': right}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        拟合决策树模型\n",
    "        \"\"\"\n",
    "        self.tree_ = self._grow_tree(X, y, depth=0)\n",
    "\n",
    "    def _predict_sample(self, x, node):\n",
    "        \"\"\"\n",
    "        预测单个样本\n",
    "        \"\"\"\n",
    "        if 'label' in node:\n",
    "            return node['label']\n",
    "        else:\n",
    "            feature, threshold = node['feature'], node['threshold']\n",
    "            if x[feature] <= threshold:\n",
    "                return self._predict_sample(x, node['left'])\n",
    "            else:\n",
    "                return self._predict_sample(x, node['right'])\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        预测整个数据集\n",
    "        \"\"\"\n",
    "        return np.array([self._predict_sample(x, self.tree_) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        根据信息增益计算最佳分割特征和阈值\n",
    "        \"\"\"\n",
    "        best_gain = -np.inf\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                mask = X[:, feature] <= threshold\n",
    "                y_left, y_right = y[mask], y[~mask]\n",
    "                gain = self._information_gain(y, y_left, y_right)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        \"\"\"\n",
    "        计算熵\n",
    "        \"\"\"\n",
    "        n_samples = len(y)\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / n_samples\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy\n",
    "\n",
    "    def _information_gain(self, y, y_left, y_right):\n",
    "        \"\"\"\n",
    "        计算信息增益\n",
    "        \"\"\"\n",
    "        p_left = len(y_left) / len(y)\n",
    "        p_right = len(y_right) / len(y)\n",
    "        gain = self._entropy(y) - (p_left * self._entropy(y_left) + p_right * self._entropy(y_right))\n",
    "        return gain\n",
    "\n",
    "    def _grow_tree(self, X, y, depth):\n",
    "        \"\"\"\n",
    "        递归地生成决策树\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        if n_labels == 1 or n_samples == 0 or depth == self.max_depth:\n",
    "            return self._create_leaf(y)\n",
    "        else:\n",
    "            feature, threshold = self._best_split(X, y)\n",
    "            mask = X[:, feature] <= threshold\n",
    "            left = self._grow_tree(X[mask], y[mask], depth + 1)\n",
    "            right = self._grow_tree(X[~mask], y[~mask], depth + 1)\n",
    "            return self._create_node(feature, threshold, left, right)\n",
    "\n",
    "    def _create_leaf(self, y):\n",
    "        return {'label': np.argmax(np.bincount(y))}\n",
    "\n",
    "    def _create_node(self, feature, threshold, left, right):\n",
    "        return {'feature': feature, 'threshold': threshold, 'left': left, 'right': right}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        拟合决策树模型\n",
    "        \"\"\"\n",
    "        self.tree_ = self._grow_tree(X, y, depth=0)\n",
    "\n",
    "    def _predict_sample(self, x, node):\n",
    "        \"\"\"\n",
    "        预测单个样本\n",
    "        \"\"\"\n",
    "        if 'label' in node:\n",
    "            return node['label']\n",
    "        else:\n",
    "            feature, threshold = node['feature'], node['threshold']\n",
    "            if x[feature] <= threshold:\n",
    "                return self._predict_sample(x, node['left'])\n",
    "            else:\n",
    "                return self._predict_sample(x, node['right'])\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        预测整个数据集\n",
    "        \"\"\"\n",
    "        return np.array([self._predict_sample(x, self.tree_) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform Cost Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, name, neighbors=None):\n",
    "        self.name = name\n",
    "        self.neighbors = neighbors if neighbors else {}\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.name < other.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_path(came_from, current_node):\n",
    "    path = [current_node.name]\n",
    "    while current_node in came_from:\n",
    "        current_node = came_from[current_node]\n",
    "        path.append(current_node.name)\n",
    "    return path[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_cost_search(start, goal):\n",
    "    open_set = [(0, start)]  # Set the initial cost to 0\n",
    "    heapq.heapify(open_set)\n",
    "    came_from = {}\n",
    "    g_cost = {start: 0}\n",
    "    node_expansion_order = 0\n",
    "\n",
    "    while open_set:\n",
    "        _, current = heapq.heappop(open_set)\n",
    "        node_expansion_order += 1\n",
    "        print(f\"Node expansion order: {node_expansion_order}\\tNode: {current.name}\")\n",
    "\n",
    "        if current == goal:\n",
    "            return reconstruct_path(came_from, current)\n",
    "\n",
    "        for neighbor, cost in current.neighbors.items():\n",
    "            tentative_g_cost = g_cost[current] + cost\n",
    "            if tentative_g_cost < g_cost.get(neighbor, float('inf')):\n",
    "                came_from[neighbor] = current\n",
    "                g_cost[neighbor] = tentative_g_cost\n",
    "                heapq.heappush(open_set, (tentative_g_cost, neighbor))  # f_cost = g_cost\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node expansion order: 1\tNode: H\n",
      "Node expansion order: 2\tNode: C\n",
      "Node expansion order: 3\tNode: A\n",
      "Node expansion order: 4\tNode: B\n",
      "Node expansion order: 5\tNode: Q\n",
      "Node expansion order: 6\tNode: G\n",
      "Path from H to G: H -> A -> Q -> G\n"
     ]
    }
   ],
   "source": [
    "H = Node(\"H\")\n",
    "A = Node(\"A\")\n",
    "B = Node(\"B\")\n",
    "C = Node(\"C\")\n",
    "D = Node(\"D\")\n",
    "E = Node(\"E\")\n",
    "F = Node(\"F\")\n",
    "G = Node(\"G\")\n",
    "Q = Node(\"Q\")\n",
    "U = Node(\"U\")\n",
    "\n",
    "H.neighbors = {A: 1.8, B: 1.8,C: 1.7}\n",
    "A.neighbors = {H: 1.8, B: 1,Q: 0.2}\n",
    "B.neighbors = {H: 1.8, A: 1}\n",
    "C.neighbors = {H: 1.7, U: 1.5}\n",
    "Q.neighbors = {A: 0.2, G:0.9}\n",
    "G.neighbors = {U: 0.7, Q: 0.9}\n",
    "\n",
    "path = uniform_cost_search(H, G)\n",
    "print(\"Path from H to G:\", \" -> \".join(path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Star Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, name, neighbors=None, h_cost=0):\n",
    "        self.name = name\n",
    "        self.neighbors = neighbors if neighbors else {}\n",
    "        self.h_cost = h_cost\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.name < other.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_path(came_from, current_node):\n",
    "    path = [current_node.name]\n",
    "    while current_node in came_from:\n",
    "        current_node = came_from[current_node]\n",
    "        path.append(current_node.name)\n",
    "    return path[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_star_search(start, goal):\n",
    "    open_set = [(start.h_cost, start)]\n",
    "    heapq.heapify(open_set)\n",
    "    came_from = {}\n",
    "    g_cost = {start: 0}\n",
    "    node_expansion_order = 0\n",
    "\n",
    "    while open_set:\n",
    "        _, current = heapq.heappop(open_set)\n",
    "        node_expansion_order += 1\n",
    "        print(f\"Node expansion order: {node_expansion_order}\\tNode: {current.name}\")\n",
    "\n",
    "        if current == goal:\n",
    "            print(g_cost)\n",
    "            return reconstruct_path(came_from, current)\n",
    "\n",
    "        for neighbor, cost in current.neighbors.items():\n",
    "            tentative_g_cost = g_cost[current] + cost\n",
    "            if tentative_g_cost < g_cost.get(neighbor, float('inf')):\n",
    "                came_from[neighbor] = current\n",
    "                g_cost[neighbor] = tentative_g_cost\n",
    "                f_cost = tentative_g_cost + neighbor.h_cost\n",
    "                heapq.heappush(open_set, (f_cost, neighbor))\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node expansion order: 1\tNode: H\n",
      "Node expansion order: 2\tNode: B\n",
      "Node expansion order: 3\tNode: A\n",
      "Node expansion order: 4\tNode: Q\n",
      "Node expansion order: 5\tNode: G\n",
      "{<__main__.Node object at 0x00000211B9EEE8F0>: 0, <__main__.Node object at 0x00000211B9ECE1A0>: 1.8, <__main__.Node object at 0x00000211B9ECE170>: 1.8, <__main__.Node object at 0x00000211B9EEF7F0>: 1.7, <__main__.Node object at 0x00000211B9EEF2E0>: 2.0, <__main__.Node object at 0x00000211B9EEE7D0>: 2.9}\n",
      "Path from H to G: HAQG\n"
     ]
    }
   ],
   "source": [
    "H = Node(\"H\",h_cost=2)\n",
    "A = Node(\"A\",h_cost=1)\n",
    "B = Node(\"B\",h_cost=0.8)\n",
    "C = Node(\"C\",h_cost=1.5)\n",
    "G = Node(\"G\",h_cost=0)\n",
    "Q = Node(\"Q\",h_cost=0.75)\n",
    "U = Node(\"U\",h_cost=0.5)\n",
    "\n",
    "H.neighbors = {A: 1.8, B: 1.8,C: 1.7}\n",
    "A.neighbors = {H: 1.8, B: 1,Q: 0.2}\n",
    "B.neighbors = {H: 1.8, A: 1}\n",
    "C.neighbors = {H: 1.7, U: 1.5}\n",
    "Q.neighbors = {A: 0.2, G:0.9}\n",
    "G.neighbors = {U: 0.7, Q: 0.9}\n",
    "\n",
    "path = a_star_search(H, G)\n",
    "print(\"Path from H to G:\", \"\".join(path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyans Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "from pgmpy.inference import VariableElimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BayesianNetwork([('D', 'H'), ('D', 'C'), ('F', 'C')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPD_D = TabularCPD(variable='D', variable_card=2, values=[[0.7], [0.3]])\n",
    "CPD_F = TabularCPD(variable='F', variable_card=2, values=[[0.6], [0.4]])\n",
    "CPD_H_given_D = TabularCPD(variable='H', variable_card=2, values=[[0.9,0.5], [0.1, 0.5]],\n",
    "                          evidence=['D'], evidence_card=[2])\n",
    "CPD_C_given_FD = TabularCPD(variable='C', variable_card=2,\n",
    "                            values=[[0.9, 0.1, 0.2, 0],\n",
    "                                    [0.1, 0.9, 0.8, 1]],\n",
    "                            evidence=['D', 'F'],\n",
    "                            evidence_card=[2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_cpds(CPD_D, CPD_F, CPD_H_given_D, CPD_C_given_FD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.check_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(H=1): 0.2200\n"
     ]
    }
   ],
   "source": [
    "inference = VariableElimination(model)\n",
    "result = inference.query(variables=['H'], joint=False)\n",
    "prob_H_1 = result['H'].values[1]\n",
    "print(f\"P(H=1): {prob_H_1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(D=1|H=1): 0.6818\n"
     ]
    }
   ],
   "source": [
    "# Perform inference on the Bayesian model\n",
    "inference = VariableElimination(model)\n",
    "# Query the probability of a forest fire given there is a campfire (P(F=1|C=1))\n",
    "result = inference.query(variables=['D'], evidence={'H': 1}, joint=False)\n",
    "prob_D_1_given_H_1 = result['D'].values[1]\n",
    "print(f\"P(D=1|H=1): {prob_D_1_given_H_1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(F=1|C=1, S=1): 0.3226\n"
     ]
    }
   ],
   "source": [
    "# Perform inference on the Bayesian model\n",
    "inference = VariableElimination(model)\n",
    "\n",
    "# Query the probability of a forest fire given there is a campfire and you smell smoke (P(F=1|C=1, S=1))\n",
    "result = inference.query(variables=['D'], evidence={'C': 1, 'F': 1}, joint=False)\n",
    "prob_F_1_given_C_1_S_1 = result['D'].values[1]\n",
    "\n",
    "print(f\"P(F=1|C=1, S=1): {prob_F_1_given_C_1_S_1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at position 0, the forward utility is -0.8 and the backward utility is 0.8 \n",
      "Go Backward\n",
      "at position 1, the forward utility is 9.1 and the backward utility is 1.9 \n",
      "Go forward\n",
      "After one iteration, the utilities are: [1.4, 3.55, 10]\n",
      "The agent tries to take these actions:\n",
      "['B', 'F', 'N/A']\n"
     ]
    }
   ],
   "source": [
    "rewards = [1, -1, 10]\n",
    "utilities = rewards.copy()\n",
    "discount_factor = 0.5\n",
    "possible_actions = [\"F\", \"B\"]\n",
    "best_actions = []\n",
    "\n",
    "def action_expected_utility(action, current_position, current_utilities):\n",
    "    forward_position = min(current_position + 1, len(current_utilities) - 1)\n",
    "    backward_position = max(current_position - 1, 0)\n",
    "\n",
    "    if action == \"forward\":\n",
    "        return 0.9 * current_utilities[forward_position] + 0.1 * current_utilities[backward_position]\n",
    "    else:\n",
    "        return 0.1 * current_utilities[forward_position] + 0.9 * current_utilities[backward_position]\n",
    "\n",
    "new_utilities = []\n",
    "\n",
    "for position in range(len(utilities)):\n",
    "    if position == 2: # Goal reached, game is finished\n",
    "        new_utilities.append(utilities[position])\n",
    "        best_actions.append(\"N/A\")\n",
    "    else:\n",
    "        forward_utility = action_expected_utility(\"forward\", position, utilities)\n",
    "        backward_utility = action_expected_utility(\"backward\", position, utilities)\n",
    "        print(f\"at position {position}, the forward utility is {forward_utility} and the backward utility is {backward_utility} \")\n",
    "        \n",
    "        # Select the best action based on the utilities\n",
    "        if forward_utility > backward_utility:\n",
    "            best_action = \"F\"\n",
    "            print(\"Go forward\")\n",
    "        else:\n",
    "            best_action = \"B\"\n",
    "            print(\"Go Backward\")\n",
    "\n",
    "        new_utility = rewards[position] + discount_factor * max(forward_utility, backward_utility)\n",
    "        new_utilities.append(new_utility)\n",
    "        best_actions.append(best_action)\n",
    "\n",
    "print(f\"After one iteration, the utilities are: {new_utilities}\")\n",
    "print(\"The agent tries to take these actions:\")\n",
    "print(best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After one iteration, the utilities are:\n",
      "[0.8500000000000001, -0.95, 6.61]\n",
      "[3.5, 6.76, 10]\n",
      "\n",
      "The agent tries to take these actions:\n",
      "['D', 'U', 'D']\n",
      "['D', 'R', 'G']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "row,col = (0,0)\n",
    "nrows = 2\n",
    "ncols = 2\n",
    "neigh = {\"U\": [max(row - 1, 0), col],\n",
    "             \"D\": [min(row + 1, nrows-1), col],\n",
    "             \"L\": [row, max(col - 1, 0)],\n",
    "             \"R\": [row, min(col + 1, ncols-1)],\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'U': [0, 0], 'D': [1, 0], 'L': [0, 0], 'R': [0, 1]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The utility of cell row 1, column 2 is -0.36\n",
      "The utility of cell row 1, column 3 is 13.18\n",
      "The utility of cell row 2, column 1 is 1.91\n",
      "The utility of cell row 2, column 2 is 1.21\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# rewards and transition model\n",
    "rewards = np.array([[-0.5, -0.5, -0.5], [2, -0.5, 10]])\n",
    "transition_probs = {'up': 0.8, 'left': 0.1, 'right': 0.1}\n",
    "gamma = 0.9\n",
    "\n",
    "# initializing utilities\n",
    "utilities = rewards.copy()\n",
    "\n",
    "# value iteration for one round\n",
    "for row in range(2):\n",
    "    for col in range(3):\n",
    "        if row == 1 and col == 2:\n",
    "            continue  # goal state\n",
    "\n",
    "        actions = []\n",
    "\n",
    "        if row == 0:  # can only move up with 80% probability\n",
    "            actions.append(transition_probs['up'] * (rewards[row + 1, col] + gamma * utilities[row + 1, col]))\n",
    "\n",
    "        if col > 0:  # can move left\n",
    "            actions.append(transition_probs['left'] * (rewards[row, col - 1] + gamma * utilities[row, col - 1]))\n",
    "\n",
    "        if col < 2:  # can move right\n",
    "            actions.append(transition_probs['right'] * (rewards[row, col + 1] + gamma * utilities[row, col + 1]))\n",
    "\n",
    "        utilities[row, col] = rewards[row, col] + gamma * max(actions)\n",
    "\n",
    "print(f'The utility of cell row 1, column 2 is {utilities[0, 1]:.2f}')\n",
    "print(f'The utility of cell row 1, column 3 is {utilities[0, 2]:.2f}')\n",
    "print(f'The utility of cell row 2, column 1 is {utilities[1, 0]:.2f}')\n",
    "print(f'The utility of cell row 2, column 2 is {utilities[1, 1]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.395  6.655 17.155]\n",
      " [ 0.895 -0.905  6.655]]\n",
      "[[ 6.87215 11.77015 22.95055]\n",
      " [ 1.86295  4.9711  11.77015]]\n",
      "[[10.6421735 16.471795  27.5837095]\n",
      " [ 4.895347   9.201487  16.471795 ]]\n",
      "[[14.30027363 20.18840467 31.34273239]\n",
      " [ 7.99049875 13.28273518 20.18840467]]\n",
      "[[17.25479625 23.26221349 34.38372374]\n",
      " [10.99164318 16.57175267 23.26221349]]\n",
      "[[19.7380416  25.74773883 36.84988031]\n",
      " [13.41491104 19.33164081 25.74773883]]\n",
      "[[21.74571395 27.77176149 38.84921032]\n",
      " [15.45123762 21.56301045 27.77176149]]\n",
      "[[23.38627966 29.41210237 40.47088996]\n",
      " [17.09758499 23.3857382  29.41210237]]\n",
      "[[24.71549635 30.74375721 41.78612999]\n",
      " [18.44283779 24.86258557 30.74375721]]\n",
      "[[25.79536059 31.82364629 42.85295174]\n",
      " [19.62545628 26.06229874 31.82364629]]\n",
      "[[26.67931639 32.69973214 43.71825342]\n",
      " [20.58643755 27.04344456 32.69973214]]\n",
      "[[27.39658652 33.41105247 44.42011835]\n",
      " [21.37241856 27.83956241 33.41105247]]\n",
      "[[27.97947545 33.98804583 44.98947994]\n",
      " [22.01017772 28.48647017 33.98804583]]\n",
      "[[28.45230899 34.45620787 45.45134968]\n",
      " [22.52841131 29.01123312 34.45620787]]\n",
      "[[28.83602668 34.83598275 45.82603048]\n",
      " [22.94879565 29.43708539 34.83598275]]\n",
      "[[29.14729919 35.14407963 46.12998039]\n",
      " [23.28994388 29.78253764 35.14407963]]\n",
      "[[29.39983228 35.39401427 46.37655305]\n",
      " [23.56668403 30.06279945 35.39401427]]\n",
      "[[29.60469184 35.59677015 46.57657948]\n",
      " [23.79120051 30.29015312 35.59677015]]\n",
      "[[29.77088255 35.76125101 46.73884654]\n",
      " [23.97333251 30.47459186 35.76125101]]\n",
      "[[29.90570065 35.89468278 46.8704821 ]\n",
      " [24.12108557 30.62421324 35.89468278]]\n",
      "[[30.0150693  36.0029263  46.97726856]\n",
      " [24.24094659 30.74559075 36.0029263 ]]\n",
      "[[30.10379213 36.09073653 47.06389673]\n",
      " [24.33818158 30.8440555  36.09073653]]\n",
      "[[30.17576664 36.16197064 47.13417193]\n",
      " [24.41706125 30.92393293 36.16197064]]\n",
      "[[30.23415437 36.21975776 47.19118115]\n",
      " [24.48105071 30.98873173 36.21975776]]\n",
      "[[30.28152015 36.26663628 47.23742863]\n",
      " [24.53296074 31.04129835 36.26663628]]\n",
      "[[30.31994459 36.30466546 47.27494588]\n",
      " [24.57507162 31.08394186 36.30466546]]\n",
      "[[30.35111558 36.3355158  47.30538092]\n",
      " [24.60923315 31.11853547 36.3355158 ]]\n",
      "[[30.37640236 36.36054246 47.33007069]\n",
      " [24.63694594 31.14659878 36.36054246]]\n",
      "[[30.3969157  36.38084478 47.35009972]\n",
      " [24.65942733 31.16936452 36.38084478]]\n",
      "[[30.4135567  36.3973146  47.36634783]\n",
      " [24.67766487 31.18783274 36.3973146 ]]\n",
      "[[30.42705635 36.41067538 47.37952875]\n",
      " [24.69245967 31.20281467 36.41067538]]\n",
      "[[30.43800764 36.42151402 47.39022148]\n",
      " [24.70446163 31.21496843 36.42151402]]\n",
      "[[30.44689164 36.43030663 47.39889573]\n",
      " [24.71419796 31.2248279  36.43030663]]\n",
      "[[30.45409859 36.43743944 47.40593252]\n",
      " [24.72209634 31.23282618 36.43743944]]\n",
      "[[30.45994506 36.44322577 47.41164096]\n",
      " [24.72850372 31.23931461 36.44322577]]\n",
      "[[30.46468789 36.44791981 47.41627181]\n",
      " [24.73370158 31.24457821 36.44791981]]\n",
      "[[30.46853541 36.45172775 47.42002849]\n",
      " [24.73791822 31.24884819 36.45172775]]\n",
      "[[30.47165662 36.45481685 47.42307601]\n",
      " [24.74133888 31.25231211 36.45481685]]\n",
      "[[30.47418863 36.45732282 47.42554824]\n",
      " [24.74411382 31.25512215 36.45732282]]\n",
      "[[30.47624267 36.45935573 47.42755379]\n",
      " [24.74636492 31.25740173 36.45935573]]\n",
      "[[30.47790897 36.46100488 47.42918074]\n",
      " [24.74819108 31.25925098 36.46100488]]\n",
      "[[30.47926071 36.46234272 47.43050057]\n",
      " [24.74967251 31.26075115 36.46234272]]\n",
      "[[30.48035729 36.46342802 47.43157126]\n",
      " [24.75087429 31.26196813 36.46342802]]\n",
      "[[30.48124686 36.46430844 47.43243983]\n",
      " [24.75184921 31.26295538 36.46430844]]\n",
      "[[30.4819685  36.46502266 47.43314444]\n",
      " [24.75264009 31.26375626 36.46502266]]\n",
      "[[30.48255392 36.46560206 47.43371603]\n",
      " [24.75328168 31.26440596 36.46560206]]\n",
      "[[30.48302883 36.46607208 47.43417973]\n",
      " [24.75380215 31.26493302 36.46607208]]\n",
      "[[30.48341409 36.46645338 47.43455589]\n",
      " [24.75422437 31.26536058 36.46645338]]\n",
      "[[30.48372662 36.46676269 47.43486105]\n",
      " [24.75456688 31.26570743 36.46676269]]\n",
      "[[30.48398016 36.46701362 47.4351086 ]\n",
      " [24.75484474 31.2659888  36.46701362]]\n",
      "[[30.48418583 36.46721718 47.43530941]\n",
      " [24.75507015 31.26621706 36.46721718]]\n",
      "[[30.48435268 36.46738231 47.43547233]\n",
      " [24.75525301 31.26640223 36.46738231]]\n",
      "[[30.48448804 36.46751627 47.43560448]\n",
      " [24.75540135 31.26655245 36.46751627]]\n",
      "[[30.48459784 36.46762495 47.43571169]\n",
      " [24.75552168 31.2666743  36.46762495]]\n",
      "[[30.48468691 36.46771311 47.43579866]\n",
      " [24.7556193  31.26677316 36.46771311]]\n",
      "[[30.48475917 36.46778462 47.43586922]\n",
      " [24.7556985  31.26685335 36.46778462]]\n",
      "[[30.48481779 36.46784264 47.43592645]\n",
      " [24.75576274 31.26691841 36.46784264]]\n",
      "[[30.48486535 36.4678897  47.43597288]\n",
      " [24.75581486 31.26697118 36.4678897 ]]\n",
      "[[30.48490392 36.46792788 47.43601055]\n",
      " [24.75585713 31.267014   36.46792788]]\n",
      "[[30.48493522 36.46795885 47.4360411 ]\n",
      " [24.75589143 31.26704873 36.46795885]]\n",
      "[[30.4849606  36.46798398 47.43606589]\n",
      " [24.75591925 31.2670769  36.46798398]]\n",
      "[[30.4849812  36.46800436 47.436086  ]\n",
      " [24.75594182 31.26709976 36.46800436]]\n",
      "[[30.48499791 36.4680209  47.43610231]\n",
      " [24.75596013 31.2671183  36.4680209 ]]\n",
      "[[30.48501146 36.46803431 47.43611555]\n",
      " [24.75597499 31.26713334 36.46803431]]\n",
      "[[30.48502245 36.46804519 47.43612628]\n",
      " [24.75598704 31.26714554 36.46804519]]\n",
      "[[30.48503137 36.46805402 47.43613499]\n",
      " [24.75599681 31.26715544 36.46805402]]\n",
      "[[30.48503861 36.46806118 47.43614205]\n",
      " [24.75600474 31.26716347 36.46806118]]\n",
      "[[30.48504448 36.46806699 47.43614779]\n",
      " [24.75601117 31.26716998 36.46806699]]\n",
      "[[30.48504924 36.4680717  47.43615244]\n",
      " [24.75601639 31.26717527 36.4680717 ]]\n",
      "[[30.4850531  36.46807553 47.43615621]\n",
      " [24.75602063 31.26717956 36.46807553]]\n",
      "[[30.48505624 36.46807863 47.43615927]\n",
      " [24.75602406 31.26718303 36.46807863]]\n",
      "[[30.48505878 36.46808114 47.43616175]\n",
      " [24.75602685 31.26718585 36.46808114]]\n",
      "[[30.48506084 36.46808319 47.43616376]\n",
      " [24.75602911 31.26718814 36.46808319]]\n",
      "[[30.48506251 36.46808484 47.4361654 ]\n",
      " [24.75603094 31.26719    36.46808484]]\n",
      "[[30.48506387 36.46808618 47.43616672]\n",
      " [24.75603243 31.26719151 36.46808618]]\n",
      "[[30.48506497 36.46808727 47.4361678 ]\n",
      " [24.75603363 31.26719273 36.46808727]]\n",
      "[[30.48506586 36.46808816 47.43616867]\n",
      " [24.75603461 31.26719372 36.46808816]]\n",
      "T\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "reward_grid = np.array([[2,-0.5,10],\n",
    "                        [-0.5, -0.5, -0.5]])\n",
    "\n",
    "utility_grid = np.copy(reward_grid)\n",
    "\n",
    "def expected_utility(current_utilities, action, discount=0.9, transition_probabilities=[0.8, 0.1, 0.1]):\n",
    "    up, down, left, right = \"up\", \"down\", \"left\", \"right\"\n",
    "\n",
    "    probs = np.array(transition_probabilities)\n",
    "    utilities = np.zeros_like(current_utilities)\n",
    "\n",
    "    if action == up:\n",
    "        utilities[1:, :] += current_utilities[:-1, :] * (probs[0])\n",
    "        utilities[0, :] += current_utilities[0, :] * (probs[0])\n",
    "        utilities[:, :-1] += current_utilities[:, 1:] * (probs[1])\n",
    "        utilities[:, 1:] += current_utilities[:, :-1] * (probs[2])\n",
    "\n",
    "    elif action == down:\n",
    "        utilities[:-1, :] += current_utilities[1:, :] * (probs[0])\n",
    "        utilities[-1, :] += current_utilities[-1, :] * (probs[0])\n",
    "        utilities[:, 1:] += current_utilities[:, :-1] * (probs[1])\n",
    "        utilities[:, :-1] += current_utilities[:, 1:] * (probs[2])\n",
    "\n",
    "    elif action == left:\n",
    "        utilities[:, 1:] += current_utilities[:, :-1] * (probs[0])\n",
    "        utilities[:, 0] += current_utilities[:, 0] * (probs[0])\n",
    "        utilities[1:, :] += current_utilities[:-1, :] * (probs[1])\n",
    "        utilities[:-1, :] += current_utilities[1:, :] * (probs[2])\n",
    "\n",
    "    elif action == right:\n",
    "        utilities[:, :-1] += current_utilities[:, 1:] * (probs[0])\n",
    "        utilities[:, -1] += current_utilities[:, -1] * (probs[0])\n",
    "        utilities[:-1, :] += current_utilities[1:, :] * (probs[1])\n",
    "        utilities[1:, :] += current_utilities[:-1, :] * (probs[2])\n",
    "\n",
    "    return utilities\n",
    "\n",
    "actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "for action in actions:\n",
    "    expected_u = expected_utility(utility_grid, action)\n",
    "    utility_grid = reward_grid + 0.9 * expected_u\n",
    "\n",
    "def value_iteration(reward_grid):\n",
    "    utility_grid = np.copy(reward_grid)\n",
    "    threshold = 1e-6\n",
    "\n",
    "    while True:\n",
    "        prev_utility_grid = np.copy(utility_grid)\n",
    "        action_values = []\n",
    "\n",
    "        for action in actions:\n",
    "            action_values.append(expected_utility(prev_utility_grid, action))\n",
    "\n",
    "        utility_grid = reward_grid + 0.9 * np.max(action_values, axis=0)\n",
    "        print(utility_grid)\n",
    "\n",
    "        diff = np.max(np.abs(utility_grid - prev_utility_grid))\n",
    "        if diff < threshold:\n",
    "            break\n",
    "\n",
    "    return utility_grid\n",
    "\n",
    "utility_grid = value_iteration(reward_grid)\n",
    "\n",
    "will_reach_goal = (utility_grid[1, 2] > utility_grid[1, 1])\n",
    "print(\"T\" if will_reach_goal else \"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrossOVer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_point_crossover(a, b, point):\n",
    "    return a[:point] + b[point:], b[:point] + a[point:]\n",
    "\n",
    "def swap_mutation(individual, p1, p2):\n",
    "    individual_list = list(individual)\n",
    "    individual_list[p1], individual_list[p2] = individual_list[p2], individual_list[p1]\n",
    "    return ''.join(individual_list)\n",
    "\n",
    "def decode_binary(binary_str):\n",
    "    return int(binary_str, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A' = 1100010 , Decimal value = 98\n",
      "B' = 1011001 , Decimal value = 89\n"
     ]
    }
   ],
   "source": [
    "A = '1101001'\n",
    "B = '1010010'\n",
    "crossover_point = 3\n",
    "mutation_points = [(2, 4), (3, 6)]\n",
    "\n",
    "A_prime, B_prime = single_point_crossover(A, B, crossover_point)\n",
    "A_prime = swap_mutation(A_prime, *mutation_points[0])\n",
    "B_prime = swap_mutation(B_prime, *mutation_points[1])\n",
    "\n",
    "decimal_A_prime = decode_binary(A_prime)\n",
    "decimal_B_prime = decode_binary(B_prime)\n",
    "\n",
    "print(\"A' =\", A_prime, \", Decimal value =\", decimal_A_prime)\n",
    "print(\"B' =\", B_prime, \", Decimal value =\", decimal_B_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
